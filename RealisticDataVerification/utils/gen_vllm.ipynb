{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "sub_task = ['metamath']\n",
    "data_path = r'/TCotMechanism/RealisticDataVerification/dataset'\n",
    "dataset_split = \"test\"\n",
    "\n",
    "\n",
    "if sub_task is None:\n",
    "    dataset = load_dataset(data_path, split=dataset_split)\n",
    "else:\n",
    "    all_test_dataset = []\n",
    "    for task in sub_task:\n",
    "        ds = load_dataset(data_path, data_dir=task, split=dataset_split)\n",
    "        print(f\"{data_path}/{task}/{dataset_split}\")\n",
    "        for k,v in ds[0].items():\n",
    "            print(\"-\"*100)\n",
    "            print(k,end=':\\t')\n",
    "            print(v)\n",
    "        print(\"+\"*100)\n",
    "        all_test_dataset.append(ds)\n",
    "        \n",
    "    dataset = concatenate_datasets(all_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"type\"] == \"gsm8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '18',\n",
       " 'instruction': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nJanetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\\n\\n### Response: Let's think step by step.\",\n",
       " 'type': 'gsm8k',\n",
       " 'input': ''}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def modify_output(example):\n",
    "    example['instruction'] =  example['instruction'].replace(\"Let's think step by step.\",\"The answer is: \")\n",
    "    return example\n",
    "# dataset = dataset.map(modify_output)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_list, batch_size=1):\n",
    "    n = len(data_list) // batch_size\n",
    "    batch_data = []\n",
    "    for i in range(n-1):\n",
    "        start = i * batch_size\n",
    "        end = (i+1)*batch_size\n",
    "        batch_data.append(data_list[start:end])\n",
    "\n",
    "    last_start = (n-1) * batch_size\n",
    "    last_end = sys.maxsize\n",
    "    batch_data.append(data_list[last_start:last_end])\n",
    "    return batch_data\n",
    "\n",
    "batch_size = 16\n",
    "batch_dataset_query = batch_data(dataset[\"instruction\"], batch_size=batch_size)\n",
    "batch_dataset_answer = batch_data(dataset[\"output\"], batch_size=batch_size)\n",
    "batch_dataset_task = batch_data(dataset[\"type\"], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "model_path = '/.../Meta-Llama-3.1-8B'\n",
    "model_path = '/metamath-LoRA-Qwen2.5-3B/checkpoint-1000'\n",
    "\n",
    "bits=16\n",
    "quant_type='nf4'\n",
    "double_quant=True\n",
    "bf16=True\n",
    "compute_dtype = (torch.bfloat16 if bf16 else torch.float32)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit= bits == 4,\n",
    "            load_in_8bit= bits == 8,\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=double_quant,\n",
    "            bnb_4bit_quant_type=quant_type,\n",
    "        ) if bits in [4, 8] else None,\n",
    "        torch_dtype=compute_dtype,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map='auto'\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512  # input max\n",
    "MAX_NEW_TOKENS = 1024  # generate max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "def batch_generate(texts):\n",
    "    predictions = []\n",
    "    for i in tqdm(range(len(texts))):\n",
    "        batch = texts[i]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        gen_config = GenerationConfig(\n",
    "        max_new_tokens=MAX_NEW_TOKENS,          \n",
    "        temperature=0.01,             \n",
    "        do_sample=False,             # Disable Random Sampling (Deterministic Mathematical Requirements)\n",
    "    )\n",
    "        \n",
    "        with torch.no_grad(), torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                generation_config=gen_config\n",
    "            )\n",
    "\n",
    "        preds = tokenizer.batch_decode(\n",
    "            outputs,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        predictions.append(preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# excute test\n",
    "num = 82\n",
    "predictions = batch_generate(batch_dataset_query[-num:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = r'/.../metamath_response.jsonl'\n",
    "for i in range(num):\n",
    "    batch_q = batch_dataset_query[-num:][i]\n",
    "    batch_p = predictions[i]\n",
    "    batch_a = batch_dataset_answer[-num:][i]\n",
    "    batch_t = batch_dataset_task[-num:][i]\n",
    "    for j in range(batch_size):\n",
    "        query = batch_q[j]\n",
    "        pre = batch_p[j]\n",
    "        answer = batch_a[j]\n",
    "        task = batch_t[j]\n",
    "        with open(output_file, 'a') as f:\n",
    "            json.dump({'type': task, 'query': query, 'output': pre, 'answer': answer}, f)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "A merchant wants to make a choice of purchase between 2 purchase plans: jewelry worth $5,000 or electronic gadgets worth $8,000. His financial advisor speculates that the jewelry market will go up 2.5% while the electronic gadgets market will rise 1.2% within the same month. If the merchant is looking to maximize profit at the end of this month by making a choice, how much profit would this be?\n",
      "\n",
      "### Response: Let's think step by step. The jewelry market is expected to go up by 2.5%, so the value of the jewelry will increase by 2.5% of $5,000, which is 0.025 * $5,000 = ...\n",
      "The electronic gadgets market is expected to go up by 1.2%, so the value of the electronic gadgets will increase by 1.2% of $8,000, which is 0.012 * $8,000 = ...\n",
      "If the merchant chooses to purchase the jewelry, he will make a profit of $125 - $5,000 = ...\n",
      "If the merchant chooses to purchase the electronic gadgets, he will make a profit of $96 - $8,000 = ...\n",
      "Therefore, the merchant should choose to purchase the electronic gadgets to maximize his profit.\n",
      "#### 96\n",
      "The answer is: 96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = batch_dataset_query[-num:][0][-1]#.replace(\"Let's think step by step.\",\"The Answer is \")\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# setting\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# decode output\n",
    "# {\"type\": \"gsm8k\", \"query\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThere are some oranges in a basket. Ana spends 3 minutes peeling an orange and Jane spends 4 minutes doing the same. If Ana and Jane start picking oranges from this basket to peel at the same time, how many more oranges will Ana have peeled than Jane after an hour?\\n\\n### Response: Let's think step by step.\", \"output\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThere are some oranges in a basket. Ana spends 3 minutes peeling an orange and Jane spends 4 minutes doing the same. If Ana and Jane start picking oranges from this basket to peel at the same time, how many more oranges will Ana have peeled than Jane after an hour?\\n\\n### Response: Let's think step by step. First of all we need to find out what number represents one minute for both girls.\\nFor example if there were only two oranges then it would be easy because each girl could pick up her own fruit without any problems but since they're not alone anymore so let me explain my reasoning behind these numbers (I hope I'm right). So first thing comes into mind when thinking about something like \\\"how much does someone spend per unit\\\" - well obviously its just their speed divided over total amount spent which gives us $latex \\\\frac{1}{2}$ or half as fast compared with other person who has no problem whatsoever getting things done quickly enough even though he might take longer breaks between tasks etc... But anyway back on track here goes nothing! We know now exactly where our starting point lies therefore next logical question becomes obvious too namely 'what happens during given period?' And again answer seems pretty straightforward considering previous information provided earlier today; every single second counts towards final result thus making sure everything gets completed within specified timeframe regardless whether you do better job overall due lack experience handling certain situations differently depending upon personal preferences regarding work ethic itself rather than actual skill level required completing particular assignment correctly once assigned properly beforehand naturally speaking otherwise wouldn't make sense having said previously mentioned factoid being true throughout entire process leading eventually\", \"answer\": \"5\"}\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pissa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
